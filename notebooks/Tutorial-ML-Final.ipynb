{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is part of Hadoop and Spark training delivered by IT-DB group\n",
    "### SPARK MLlib Hands-On Lab\n",
    "_ by Prasanth Kothuri _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext, SQLContext, SparkConf\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from collections import namedtuple\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add databricks csv package\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.5.0 pyspark-shell'\n",
    "# create sparkContext and sqlContext\n",
    "conf = SparkConf().set(\"spark.executor.memory\", \"4g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load flight data into spark DataFrame and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read train and test datasets\n",
    "flights_df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferSchema='true').load(\"../data/US_Flight_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dofM: integer (nullable = true)\n",
      " |-- dofW: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flnum: integer (nullable = true)\n",
      " |-- org_id: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest_id: integer (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- crsdeptime: integer (nullable = true)\n",
      " |-- deptime: integer (nullable = true)\n",
      " |-- depdelaymins: double (nullable = true)\n",
      " |-- crsarrtime: integer (nullable = true)\n",
      " |-- arrtime: integer (nullable = true)\n",
      " |-- arrdelay: double (nullable = true)\n",
      " |-- crselapsedtime: double (nullable = true)\n",
      " |-- dist: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+-------+-----+------+------+-------+----+----------+-------+------------+----------+-------+--------+--------------+------+\n",
      "|dofM|dofW|carrier|tailnum|flnum|org_id|origin|dest_id|dest|crsdeptime|deptime|depdelaymins|crsarrtime|arrtime|arrdelay|crselapsedtime|  dist|\n",
      "+----+----+-------+-------+-----+------+------+-------+----+----------+-------+------------+----------+-------+--------+--------------+------+\n",
      "|   1|   7|     AA| N3CGAA|  307| 11292|   DEN|  14107| PHX|      1145|   1135|         0.0|      1345|   1328|   -17.0|         120.0| 602.0|\n",
      "|   1|   7|     AA| N3CGAA|  307| 14107|   PHX|  14057| PDX|      1510|   1502|         0.0|      1701|   1653|    -8.0|         171.0|1009.0|\n",
      "|   1|   7|     AA| N3EVAA|  309| 11278|   DCA|  13303| MIA|       659|    646|         0.0|       944|    930|   -14.0|         165.0| 919.0|\n",
      "+----+----+-------+-------+-----+------+------+-------+----+----------+-------+------------+----------+-------+--------+--------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for ML Algorithm ###\n",
    "#### 1. create delayed label column ####\n",
    "_ mark flight as delayed if depdelaymins > 40 min _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def delayed(record):\n",
    "  if record:\n",
    "        if float(record) > 40:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "  else:\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "dlay = udf(lambda x: delayed(x), FloatType())\n",
    "features_df = flights_df.withColumn(\"delayed\", dlay(flights_df.depdelaymins.cast('float'))).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+-------+-----+------+------+-------+----+----------+-------+------------+----------+-------+--------+--------------+------+-------+\n",
      "|dofM|dofW|carrier|tailnum|flnum|org_id|origin|dest_id|dest|crsdeptime|deptime|depdelaymins|crsarrtime|arrtime|arrdelay|crselapsedtime|  dist|delayed|\n",
      "+----+----+-------+-------+-----+------+------+-------+----+----------+-------+------------+----------+-------+--------+--------------+------+-------+\n",
      "|   1|   7|     AA| N3CGAA|  307| 11292|   DEN|  14107| PHX|      1145|   1135|         0.0|      1345|   1328|   -17.0|         120.0| 602.0|    0.0|\n",
      "|   1|   7|     AA| N3CGAA|  307| 14107|   PHX|  14057| PDX|      1510|   1502|         0.0|      1701|   1653|    -8.0|         171.0|1009.0|    0.0|\n",
      "|   1|   7|     AA| N3EVAA|  309| 11278|   DCA|  13303| MIA|       659|    646|         0.0|       944|    930|   -14.0|         165.0| 919.0|    0.0|\n",
      "+----+----+-------+-------+-----+------+------+-------+----+----------+-------+------------+----------+-------+--------+--------------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Encode string column of labels to column of label indices ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "carrierIdx = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIdx\")\n",
    "originIdx = StringIndexer(inputCol=\"origin\", outputCol=\"originIdx\")\n",
    "destIdx = StringIndexer(inputCol=\"dest\", outputCol=\"destIdx\")\n",
    "#pipeline1 = Pipeline(stages=[carrierIdx,originIdx,destIdx])\n",
    "#f_df = pipeline1.fit(features_df).transform(features_df)\n",
    "#f_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create feature vector ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# identify the list of features\n",
    "# assembler will create a single column with vector of features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"dofM\", \"dofW\", \"crsdeptime\", \"crsarrtime\", \"crselapsedtime\", \"carrierIdx\", \"originIdx\", \"destIdx\"],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "assembler.transform(trainingData.limit(5)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose the machine learning algorithm ###\n",
    "_ Decision tree is commonly used for classification _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier parameters:\n",
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. (default: 10)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini, current: gini)\n",
      "labelCol: label column name. (default: label, current: delayed)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32, current: 9000)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 12)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: 6174923023070228847)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(labelCol=\"delayed\", featuresCol=\"features\", maxDepth=12, maxBins=9000, impurity=\"gini\")\n",
    "print(\"DecisionTreeClassifier parameters:\\n\" + dt.explainParams() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a sequence of actions / transformation to be run ###\n",
    "_ Spark MLlib represents such a sequence as pipeline _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a pipeline to run carrierIdx->originIdx->destIdx->assembler->dt\n",
    "pipeline = Pipeline(stages=[carrierIdx,originIdx,destIdx,assembler,dt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Split data into training and test ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = features_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model by running the pipeline ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply the model to the testdata to get the predictions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(prediction=0.0, count=131622), Row(prediction=1.0, count=3560)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.groupby(\"prediction\").count().collect()\n",
    "#predictions.select(\"dofM\",\"dofW\",\"carrier\",\"flnum\",\"origin\",\"dest\",\"deptime\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.105783 \n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"delayed\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
