{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is part of Hadoop and Spark training delivered by IT-DB group\n",
    "### SPARK DataFrame1 Hands-On Lab\n",
    "_ by Prasanth Kothuri _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On 1 - Construct a DataFrame from csv file\n",
    "*This demostrates how to read a csv file and construct a DataFrame*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the csv file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .csv(\"/tmp/online-retail-dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN               |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n",
      "+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On 2 - Spark Transformations - select, add, rename and drop columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|       Country|\n",
      "+--------------+\n",
      "|United Kingdom|\n",
      "|United Kingdom|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+----------------------------------+\n",
      "|StockCode|Description                       |\n",
      "+---------+----------------------------------+\n",
      "|85123A   |WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|71053    |WHITE METAL LANTERN               |\n",
      "+---------+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select single column\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"Country\")).show(2)\n",
    "# select multiple columns\n",
    "df.select(\"StockCode\",\"Description\").show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|HighValueItem|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|        false|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|        false|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selects all the original columns and adds a new column that specifies high value item\n",
    "df.selectExpr(\n",
    "  \"*\", # all original columns\n",
    "  \"(UnitPrice > 100) as HighValueItem\")\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding, renaming and dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+\n",
      "|InvoiceNo|         Description|      InvoiceValue|\n",
      "+---------+--------------------+------------------+\n",
      "|   536365|WHITE HANGING HEA...|15.299999999999999|\n",
      "|   536365| WHITE METAL LANTERN|             20.34|\n",
      "+---------+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+--------------------+------------------+\n",
      "|InvoiceNo|         Description|         LineTotal|\n",
      "+---------+--------------------+------------------+\n",
      "|   536365|WHITE HANGING HEA...|15.299999999999999|\n",
      "|   536365| WHITE METAL LANTERN|             20.34|\n",
      "+---------+--------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|InvoiceNo|         Description|\n",
      "+---------+--------------------+\n",
      "|   536365|WHITE HANGING HEA...|\n",
      "|   536365| WHITE METAL LANTERN|\n",
      "+---------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add a new column called InvoiceValue\n",
    "from pyspark.sql.functions import expr\n",
    "df_1 = df.withColumn(\"InvoiceValue\", expr(\"UnitPrice * Quantity\"))\\\n",
    "    .select(\"InvoiceNo\",\"Description\",\"InvoiceValue\")\n",
    "df_1.show(2)\n",
    "\n",
    "# rename InvoiceValue to LineTotal\n",
    "df_2 = df_1.withColumnRenamed(\"InvoiceValue\",\"LineTotal\")\n",
    "df_2.show(2)\n",
    "\n",
    "# drop a column\n",
    "df_2.drop(\"LineTotal\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On 3 - Spark Transformations - filter, sort and cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|   558777|    22501|PICNIC BASKET WIC...|     125|  7/4/2011 10:23|    20.79|      null|United Kingdom|\n",
      "|   572209|    23485|BOTANICAL GARDENS...|     120|10/21/2011 12:08|     20.8|     18102|United Kingdom|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select invoice lines with quantity > 100 and unitprice > 20\n",
    "df.where(col(\"Quantity\") > 100).where(col(\"UnitPrice\") > 20).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536378|    21212|PACK OF 72 RETROS...|     120|12/1/2010 9:37|     0.42|     14688|United Kingdom|\n",
      "|  C536379|        D|            Discount|      -1|12/1/2010 9:41|     27.5|     14527|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select invoice lines with quantity > 100 0r unitprice > 20\n",
    "df.where((col(\"Quantity\") > 100) | (col(\"UnitPrice\") > 20)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|    Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+---------------+--------+---------------+---------+----------+--------------+\n",
      "|  A563187|        B|Adjust bad debt|       1|8/12/2011 14:52|-11062.06|      null|United Kingdom|\n",
      "|  A563186|        B|Adjust bad debt|       1|8/12/2011 14:51|-11062.06|      null|United Kingdom|\n",
      "+---------+---------+---------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|   581483|    23843|PAPER CRAFT , LIT...|   80995|  12/9/2011 9:15|     2.08|     16446|United Kingdom|\n",
      "|   541431|    23166|MEDIUM CERAMIC TO...|   74215| 1/18/2011 10:01|     1.04|     12346|United Kingdom|\n",
      "|   578841|    84826|ASSTD DESIGN 3D P...|   12540|11/25/2011 15:57|      0.0|     13256|United Kingdom|\n",
      "|   542504|    37413|                null|    5568| 1/28/2011 12:03|      0.0|      null|United Kingdom|\n",
      "|   573008|    84077|WORLD WAR 2 GLIDE...|    4800|10/27/2011 12:26|     0.21|     12901|United Kingdom|\n",
      "|   554868|    22197|SMALL POPCORN HOLDER|    4300| 5/27/2011 10:52|     0.72|     13135|United Kingdom|\n",
      "|   556231|   85123A|                   ?|    4000|  6/9/2011 15:04|      0.0|      null|United Kingdom|\n",
      "|   544612|    22053|EMPIRE DESIGN ROS...|    3906| 2/22/2011 10:43|     0.82|     18087|United Kingdom|\n",
      "|   560599|    18007|ESSENTIAL BALM 3....|    3186| 7/19/2011 17:04|     0.06|     14609|United Kingdom|\n",
      "|   540815|    21108|FAIRY CAKE FLANNE...|    3114| 1/11/2011 12:55|      2.1|     15749|United Kingdom|\n",
      "|   550461|    21108|FAIRY CAKE FLANNE...|    3114| 4/18/2011 13:20|      2.1|     15749|United Kingdom|\n",
      "|   560040|    23343| came coded as 20713|    3100| 7/14/2011 14:28|      0.0|      null|United Kingdom|\n",
      "|   546139|    84988|                   ?|    3000|  3/9/2011 16:35|      0.0|      null|United Kingdom|\n",
      "|   573995|    16014|SMALL CHINESE STY...|    3000| 11/2/2011 11:24|     0.32|     16308|United Kingdom|\n",
      "|   536830|    84077|WORLD WAR 2 GLIDE...|    2880| 12/2/2010 17:38|     0.18|     16754|United Kingdom|\n",
      "|   562439|    84879|ASSORTED COLOUR B...|    2880|  8/4/2011 18:06|     1.45|     12931|United Kingdom|\n",
      "|   554272|    21977|PACK OF 60 PINK P...|    2700| 5/23/2011 13:08|     0.42|     12901|United Kingdom|\n",
      "|   543057|    84077|WORLD WAR 2 GLIDE...|    2592|  2/3/2011 10:50|     0.21|     16333|United Kingdom|\n",
      "|   542505|   79063D|                null|    2560| 1/28/2011 12:04|      0.0|      null|United Kingdom|\n",
      "|   544152|    18007|ESSENTIAL BALM 3....|    2400| 2/16/2011 12:10|     0.06|     14609|United Kingdom|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"UnitPrice desc\")).show(2)\n",
    "df.orderBy(col(\"Quantity\").desc(), col(\"UnitPrice\").asc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On 4 - Spark Transformations - aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT CustomerID)|\n",
      "+--------------------------+\n",
      "|                      4372|\n",
      "+--------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count distinct customers\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"CustomerID\")).show()\n",
    "\n",
    "# approx. distinct stock items\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|   avg_purchases|  mean_purchases|\n",
      "+----------------+----------------+\n",
      "|9.55224954743324|9.55224954743324|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# average and mean purchase quantity\n",
    "from pyspark.sql.functions import sum, count, avg, expr, mean\n",
    "df.select(\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    mean(\"Quantity\").alias(\"mean_purchases\"))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On 5 - Spark Transformations - grouping and windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "+---------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|  C542604|              -8.0|  15.173990905493518|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of items on the invoice\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)\n",
    "# grouping with expressions\n",
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# window functions\n",
    "\n",
    "# add date column\n",
    "from pyspark.sql.functions import col, to_date\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")\n",
    "\n",
    "# create a window specification\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowSpec = Window\\\n",
    "  .partitionBy(\"CustomerId\", \"date\")\\\n",
    "  .orderBy(desc(\"Quantity\"))\\\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# max purchase quantity\n",
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n",
    "\n",
    "# dense rank\n",
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "  .select(\n",
    "    col(\"CustomerId\"),\n",
    "    col(\"date\"),\n",
    "    col(\"Quantity\"),\n",
    "    purchaseRank.alias(\"quantityRank\"),\n",
    "    purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
